{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Solar Wind Exploration\n",
    "\n",
    "In the initial phase, we want to see if we can detect FTEs using unsupervised learning, by finding a manifold for the solar wind data.\n",
    "\n",
    "The initial hypothesis is the transition matrices (Markov Matrices $M$) that can be derived from Manifolder + clustering will show distinctive clusters and transitions.  We can check accuracy by looking at the label (FTE or not?), and see if this label could have been deduced from the data itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful set of python includes\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Solar Wind Data, and Run Manifolder\n",
    "\n",
    "The `dataset_2` file contains \n",
    "\n",
    "Dataset-2 (THEMIS):   a list with FTEs periods and non-FTEs periods observed by THEMIS in 2007.  These are combined into one file, randomly FTE - NonFTE - FTE - FTE, NonFTE, etcâ€¦\n",
    "\n",
    "In total there are 63 FTEs and 47 non-FTEs.\n",
    "\n",
    "The time series are separated by one blank line, and each one has 1440 points in a period of 6 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\acloninger\\GDrive\\ac2528Backup\\DocsFolder\\GitHub\\manifolder\")\n",
    "sys.path.append(r\"..\")\n",
    "\n",
    "import manifolder as mr\n",
    "from manifolder import helper as mh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# note, you must have started the notebook in the \n",
    "\n",
    "print('loading data ...')\n",
    "df = pd.read_excel('astro_data/dataset_2.xlsx', index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert values from loaded spreadsheet, into a numpy matrices\n",
    "# note that there is no need for the first value, which is time,\n",
    "# as it is not part of the manifold\n",
    "#\n",
    "# also, note the spreadsheet is missing a column name for `Unnamed: 13`, and the values above\n",
    "# this have the incorrect column labels; the first relevant vale is bx, which as a magnitude around 2\n",
    "#\n",
    "# note the final value of each row is the goal (0 or 1), and not part of z\n",
    "\n",
    "data_raw = df.values[:, 1:]\n",
    "print('first line of raw_data:\\n', data_raw[0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the data, breaking out the clusters\n",
    "# i will always point to the NaN (blank line) in the dataframe,\n",
    "# and values [i-1440:i] is the snipped\n",
    "\n",
    "snippet_len = 1440\n",
    "\n",
    "# collect all line breaks (blank lines) in csv file\n",
    "#lineBreaks = [0]\n",
    "#for i in range(data_raw.shape[0]):\n",
    "#    if data_raw[i,0] != data_raw[i,0]:  # replacement of isnan, since nan != nan\n",
    "#        lineBreaks.append(i)    \n",
    "#lineBreaks.append(data_raw.shape[0])\n",
    "#\n",
    "#num_snippet = len(lineBreaks)-1\n",
    "\n",
    "\n",
    "# callect the snippets into two groups, one for each goal (target) value, 0 or 1\n",
    "# these can be easily merged\n",
    "zs_0 = []\n",
    "zs_1 = []\n",
    "\n",
    "locallabel_0 = []\n",
    "locallabel_1 = []\n",
    "\n",
    "df.values[0,:]\n",
    "\n",
    "for i in range(snippet_len,data_raw.shape[0]+1,snippet_len+1):\n",
    "    # copy the snipped, excluding the last value, which is the goal\n",
    "    snippet = data_raw[i-snippet_len:i,:-1]\n",
    "    \n",
    "    # grab the goal value from the first row of each snippet\n",
    "    goal = data_raw[i-snippet_len,-1]\n",
    "    \n",
    "    # check to make sure each snippet does not contain NaN\n",
    "    # (should not, if parsing is correct)\n",
    "    assert ~np.isnan(snippet).any(), 'oops, snippet contains a Nan!'\n",
    "    \n",
    "    # print('snippet size',snippet.shape,'with goal',goal)\n",
    "    \n",
    "    if goal == 0:\n",
    "        zs_0.append( snippet )\n",
    "        locallabel_0.append(0)\n",
    "    elif goal == 1:\n",
    "        zs_1.append( snippet )\n",
    "        locallabel_1.append(1)\n",
    "    else:\n",
    "        assert False, 'value of goal not understood'\n",
    "\n",
    "# shuffle this lists; this should not strictly be necessary, if all the data is being used,\n",
    "# but prevents biases when shortening the list\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(zs_0)\n",
    "random.shuffle(zs_1)\n",
    "\n",
    "shorten_data = False\n",
    "\n",
    "if shorten_data:\n",
    "    zs_0 = zs_0[:10]\n",
    "    zs_1 = zs_1[:10]\n",
    "\n",
    "zs = zs_0 + zs_1\n",
    "locallabel = locallabel_0 + locallabel_1\n",
    "z_breakpoint = len(zs_0)\n",
    "\n",
    "print( '\\done!')\n",
    "print( '\\t len(zs_0):',len(zs_0))\n",
    "print( '\\t len(zs_1):',len(zs_1))\n",
    "print( '\\t len(zs):',len(zs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.plot(zs_0[i])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.plot(zs_1[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data has been parsed, now run Manifolder\n",
    "\n",
    "H = 80\n",
    "step_size = 10\n",
    "nbins = 10\n",
    "ncov = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# create manifolder object\n",
    "manifolder = mr.Manifolder(H=H,step_size=step_size,nbins=nbins, ncov=ncov)\n",
    "\n",
    "# add the data, and fit (this runs all the functions)\n",
    "manifolder.fit_transform(zs, parallel=True, use_dtw=True, dtw_stack=True, dtw_stack_dims=(1,))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\n\\t Program Executed in', str(np.round(elapsed_time, 2)), 'seconds')  # about 215 seconds (four minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will dump the dtw matrix calculated in manifolder.fit_transform() into a file\n",
    "import pickle\n",
    "try:\n",
    "    f = open(\"dtw_distmat_snippets_dim_1.pickle\", 'w+b')\n",
    "    pickle.dump(manifolder.dtw_distmat, f)\n",
    "    print(\"dumped dtw distance matrix\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will read the dtw matrix from the file\n",
    "import pickle\n",
    "try:\n",
    "    #dtw_distmat.pickle is compressed and split into 100MB zip files in Github,\n",
    "    # unzipped it is 1.5GB\n",
    "    f = open(\"dtw_distmat_snippets_dim_5.pickle\", 'rb')\n",
    "    dtw_distmat_dim_5 = pickle.load(f)\n",
    "finally:\n",
    "    f.close()\n",
    "    \n",
    "print(dtw_distmat_dim_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "test_size = .33\n",
    "val_size = 0\n",
    "\n",
    "# snippet_label = manifolder.snip_number\n",
    "num_snips_0 = int(len(zs_0))\n",
    "num_snips_1 = int(len(zs_1))\n",
    "\n",
    "num_runs = 1000\n",
    "\n",
    "cms = []\n",
    "for i in range(num_runs):\n",
    "    snip_indices_0 = np.random.permutation(num_snips_0)\n",
    "    snip_indices_1 = np.random.permutation(num_snips_1) + num_snips_0\n",
    "\n",
    "    n_train_0 = int((1-test_size-val_size)*num_snips_0)\n",
    "    n_val_0 = int(val_size*num_snips_0)\n",
    "    snip_train_0 = snip_indices_0[0:n_train_0]\n",
    "    snip_val_0 = snip_indices_0[n_train_0:n_train_0+n_val_0]\n",
    "    snip_test_0 = snip_indices_0[n_train_0+n_val_0: num_snips_0]\n",
    "\n",
    "    n_train_1 = int((1-test_size-val_size)*num_snips_1)\n",
    "    n_val_1 = int(val_size*num_snips_1)\n",
    "    snip_train_1 = snip_indices_1[0:n_train_1]\n",
    "    snip_val_1 = snip_indices_1[n_train_1:n_train_1+n_val_1]\n",
    "    snip_test_1 = snip_indices_1[n_train_1+n_val_1: num_snips_1]\n",
    "\n",
    "    # print(type(locallabel))\n",
    "    # X_train = Psi[np.isin(snippet_label, np.concatenate((snip_train_0, snip_train_1))),:]\n",
    "    # y_train = locallabel_downsample[np.isin(snippet_label, np.concatenate((snip_train_0, snip_train_1)))]\n",
    "    y_train = np.array(locallabel)[np.array(np.concatenate((snip_train_0, snip_train_1)))]\n",
    "    # X_val = Psi[np.isin(snippet_label, np.concatenate((snip_val_0, snip_val_1))),:]\n",
    "    # y_val = locallabel_downsample[np.isin(snippet_label, np.concatenate((snip_val_0, snip_val_1)))]\n",
    "    y_val = np.array(locallabel)[np.array(np.concatenate((snip_val_0, snip_val_1)))]\n",
    "    # X_test = Psi[np.isin(snippet_label, np.concatenate((snip_test_0, snip_test_1))),:]\n",
    "    # y_test = locallabel_downsample[np.isin(snippet_label, np.concatenate((snip_test_0, snip_test_1)))]\n",
    "    y_test = np.array(locallabel)[np.array(np.concatenate((snip_test_0, snip_test_1)))]\n",
    "    # specify full dtw matrix\n",
    "    dist_full = dtw_distmat_dim_5\n",
    "\n",
    "    # extract the indices for train and test\n",
    "    train_idx = np.array(np.concatenate((snip_train_0, snip_train_1)))\n",
    "    test_idx = np.array(np.concatenate((snip_test_0, snip_test_1)))\n",
    "\n",
    "    # specify distance matrix for train only\n",
    "    dist_train = np.zeros((train_idx.shape[0], train_idx.shape[0]))\n",
    "\n",
    "    for i1 in range(dist_train.shape[0]):\n",
    "        v1 = train_idx[i1]\n",
    "        for j1 in range(i1+1, dist_train.shape[0]):\n",
    "            v2 = train_idx[j1]\n",
    "            dist_train[i1,j1] = dist_full[v1,v2]\n",
    "            dist_train[j1,i1] = dist_full[v2,v1]\n",
    "\n",
    "    inds = np.argsort(dist_train,axis=1) # sort every row and assign index based on original placement in matrix\n",
    "\n",
    "\n",
    "    percentCorrect = np.zeros(25)\n",
    "    for knn in range(1,25):\n",
    "        predict_class = np.median(y_train[inds[:,1:knn+1]],axis=1) # Could replace with \"is there a label\" or class_weight\n",
    "        predict_class = predict_class.astype(int) \n",
    "        CM = sklearn.metrics.confusion_matrix(y_train.astype(int), predict_class )\n",
    "        percentCorrect[knn] = CM[1,1]/np.sum(CM[:,1])\n",
    "\n",
    "    #print(percentCorrect)\n",
    "\n",
    "    knn = np.argmax(percentCorrect)\n",
    "    #print(knn)\n",
    "\n",
    "    # specify distance matrix for test only\n",
    "    dist_test = np.zeros((test_idx.shape[0], train_idx.shape[0]))\n",
    "    #print(dist_test.shape)\n",
    "\n",
    "    for i1 in range(dist_test.shape[0]):\n",
    "        v1 = test_idx[i1]\n",
    "        for j1 in range(dist_train.shape[0]):\n",
    "            v2 = train_idx[j1]\n",
    "            dist_test[i1,j1] = dist_full[v1,v2]\n",
    "\n",
    "    inds = np.argsort(dist_test,axis=1)\n",
    "\n",
    "    predict_class = np.median(y_train[inds[:,0:knn]],axis=1)\n",
    "    predict_class_f = predict_class\n",
    "    predict_class = predict_class.astype(int) \n",
    "\n",
    "    CM = sklearn.metrics.confusion_matrix(y_test.astype(int), predict_class )\n",
    "    cms.append(CM)\n",
    "print('DTW Confusion:')\n",
    "print(cms[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsePositive = []\n",
    "falseNegative = []\n",
    "total = np.sum(cms[0], axis=0)\n",
    "print(total)\n",
    "for cm in cms:\n",
    "    falsePositive.append(cm[0,1]/total[1])\n",
    "    falseNegative.append(cm[1,0]/total[0])\n",
    "#print(falsePositive)\n",
    "#print(falseNegative)\n",
    "plt.hist(falsePositive, bins=np.linspace(0,1, num=15))#, bins = range(0,14))\n",
    "plt.show()\n",
    "plt.hist(falseNegative, bins=np.linspace(0,1, num=15))#, bins = range(0,14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will read the dtw matrix from the file (all dimensions used from original data)\n",
    "import pickle\n",
    "try:\n",
    "    #dtw_distmat.pickle is compressed and split into 100MB zip files in Github,\n",
    "    # unzipped it is 1.5GB\n",
    "    f = open(\"dtw_distmat.pickle\", 'rb')\n",
    "    manifolder.dtw_distmat = pickle.load(f)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(manifolder.dtw_distmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "numClusters = 7\n",
    "\n",
    "manifolder._clustering(numClusters=numClusters, kmns=False, distance_measure=\"dtw\")  # display\n",
    "print(manifolder.IDX.shape)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('\\n\\t Program Executed for k means clustering in', str(np.round(elapsed_time, 2)), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering data for k-means...\n",
    "\n",
    "IDX = manifolder.IDX\n",
    "cluster_lens = mh.count_cluster_lengths(IDX)\n",
    "\n",
    "# cluster_lens is a dictionary a dictonary, where each key is the cluster number (0:6),\n",
    "# and the values are a list of cluster lengths\n",
    "\n",
    "mh.show_cluster_lengths(cluster_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, index goes from 0 to 6 ...\n",
    "# can also have outlier groups in kmeans, need to check for this\n",
    "\n",
    "print(IDX.shape)\n",
    "print(np.min(IDX))\n",
    "print(np.max(IDX))\n",
    "\n",
    "IDX_max = np.max(IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mh.make_transition_matrix(IDX)\n",
    "print('\\n transition matrix:')\n",
    "print(M)\n",
    "\n",
    "M_z0 = mh.make_transition_matrix(IDX[manifolder.snip_number<z_breakpoint])\n",
    "M_z1 = mh.make_transition_matrix(IDX[manifolder.snip_number>=z_breakpoint])\n",
    "\n",
    "print('\\n z0 transition matrix:')\n",
    "print(M_z0)\n",
    "\n",
    "print('\\n z1 transition matrix:')\n",
    "print(M_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_downsample = np.empty((0,zs[0].shape[1]+1), float)\n",
    "for i in range(len(zs)):\n",
    "    x = zs[i]\n",
    "    x = x[0:x.shape[0]-H,:]\n",
    "    x = x[::step_size]\n",
    "    if i<z_breakpoint:\n",
    "        x = np.append(x,np.zeros((x.shape[0],1)),1)\n",
    "    else:\n",
    "        x = np.append(x,np.ones((x.shape[0],1)),1)\n",
    "\n",
    "    z_downsample = np.append(z_downsample,x,0)\n",
    "\n",
    "z_downsample = np.append(z_downsample, manifolder.snip_number.reshape(len(IDX),1), 1)\n",
    "z_downsample = np.append(z_downsample, IDX.reshape(len(IDX),1), 1)\n",
    "    \n",
    "z_downsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"astro_dtw_k=\"+str(numClusters)\n",
    "filename += \"_stack=\"\n",
    "if stack\n",
    "filename += \".csv\"\n",
    "\n",
    "np.savetxt('astro_dtw_k='+str(numClusters)+'.csv', z_downsample, delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "After running `fit_transform()`, use kmeans to label clusters withing all the snippets\n",
    "\n",
    "Create a transition matrix for each snippet; the zs_0 and zs_1 should have distincively different matrices, which can be used to categorize the snippet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### _cluster() function, local to make it easier to work on\n",
    "### ... note, all the individual clusters should be marked invidually?\n",
    "### but the original kmeans run run all of them together?\n",
    "###\n",
    "\n",
    "# Configuration\n",
    "numClusters = 7  # NOTE, this was previously 14 (too many!)\n",
    "intrinsicDim = manifolde.Dim  # can be varied slightly but shouldn't be much larger than Dim\n",
    "\n",
    "## Clusters\n",
    "# IDX = kmeans(Psi(:, 1:intrinsicDim), numClusters)\n",
    "\n",
    "# Python kmeans see\n",
    "# https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.vq.kmeans.html\n",
    "# scipy.cluster.vq.kmeans(obs, k_or_guess, iter=20, thresh=1e-05)\n",
    "#\n",
    "#  note, python expects each ROW to be an observation, looks the same a matlap\n",
    "#\n",
    "\n",
    "print('running k-means')\n",
    "\n",
    "kmeans = KMeans(n_clusters=numClusters).fit(manifolder.Psi[:, :intrinsicDim])\n",
    "IDX = kmeans.labels_\n",
    "\n",
    "# TODO decide how to plot multiple snips\n",
    "# think that x_ref[1,:] is just\n",
    "for snip in range(len(self.z)):\n",
    "    if snip == 0:\n",
    "        x = self.z[snip][0, :]\n",
    "        xref1 = x[::self.stepSize]  # downsample, to match the data steps\n",
    "    else:\n",
    "        x = self.z[snip][0, :]\n",
    "        x = x[::self.stepSize]\n",
    "        xref1 = np.append(xref1, x)\n",
    "\n",
    "print(xref1.shape)\n",
    "\n",
    "xs = manifolder.Psi[:, 0]\n",
    "ys = manifolder.Psi[:, 1]\n",
    "zs = manifolder.Psi[:, 2]\n",
    "\n",
    "# normalize these to amplitude one?\n",
    "print('normalizing amplitudes of Psi in Python ...')\n",
    "xs /= np.max(np.abs(xs))\n",
    "ys /= np.max(np.abs(ys))\n",
    "zs /= np.max(np.abs(zs))\n",
    "\n",
    "# xs -= np.mean(xs)\n",
    "# ys -= np.mean(ys)\n",
    "# zs -= np.mean(zs)\n",
    "\n",
    "# xs /= np.std(xs)\n",
    "# ys /= np.std(ys)\n",
    "# zs /= np.std(zs)\n",
    "\n",
    "print(xs.shape)\n",
    "\n",
    "lim = 2000\n",
    "val = xref1[:lim]\n",
    "idx = manifolder.IDX[:lim]\n",
    "\n",
    "plt.figure(figsize=[15, 3])\n",
    "\n",
    "plt.plot(xref1[:lim], color='black', label='Timeseries')\n",
    "# plt.plot(xs[:lim], linewidth=.5, label='$\\psi_0$')\n",
    "# plt.plot(ys[:lim], linewidth=.5, label='$\\psi_1$')\n",
    "# plt.plot(zs[:lim], linewidth=.5, label='$\\psi_2$')\n",
    "\n",
    "plt.plot(xs[:lim], linewidth=.5, label='psi_0')\n",
    "plt.plot(ys[:lim], linewidth=.5, label='psi_1')\n",
    "plt.plot(zs[:lim], linewidth=.5, label='psi_2')\n",
    "\n",
    "plt.plot(idx / np.max(idx) + 1, linewidth=.8, label='IDX')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# rightarrow causes an image error, when displayed in github!\n",
    "# plt.xlabel('Time $ \\\\rightarrow $')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# plt.gca().autoscale(enable=True, axis='both', tight=None )\n",
    "# plt.gca().xaxis.set_ticklabels([])\n",
    "# plt.gca().yaxis.set_ticklabels([])\n",
    "\n",
    "plt.title('Example Timeseries and Manifold Projection')\n",
    "\n",
    "print('done')\n",
    "\n",
    "###\n",
    "### additional parsing, for color graphs\n",
    "###\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "\n",
    "r = xs[:lim]\n",
    "g = ys[:lim]\n",
    "b = zs[:lim]\n",
    "\n",
    "# prevent the jump in data value\n",
    "r[:self.H] = r[self.H]\n",
    "g[:self.H] = g[self.H]\n",
    "b[:self.H] = b[self.H]\n",
    "\n",
    "r -= np.min(r)\n",
    "r /= np.max(r)\n",
    "\n",
    "g -= np.min(g)\n",
    "g /= np.max(g)\n",
    "\n",
    "b -= np.min(b)\n",
    "b /= np.max(b)\n",
    "\n",
    "plt.figure(figsize=[15, 3])\n",
    "\n",
    "for i in range(lim - 1):\n",
    "    col = [r[i], g[i], b[i]]\n",
    "    plt.plot([i, i + 1], [val[i], val[i + 1]], color=col)\n",
    "\n",
    "plt.title('data, colored according to Psi (color three-vector)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering data ...\n",
    "\n",
    "IDX = manifolder.IDX\n",
    "\n",
    "cluster_lens = mh.count_cluster_lengths(IDX)\n",
    "\n",
    "# cluster_lens is a dictionary a dictonary, where each key is the cluster number (0:6),\n",
    "# and the values are a list of clusner lengths\n",
    "\n",
    "mh.show_cluster_lens(cluster_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transition (Markov) Matrix\n",
    "\n",
    "The system can be though of as in one particular \"state\" (cluster value) at any given time.  This state $S$ can be though of as a column vector with $C$ dimensions, similar to states in quantum mechanic, where the column vector plays the role of the transition matrix.\n",
    "\n",
    "Time evolution is this given by the tranistion matrix $M$, which is a Markov matrix (all columns sum to one, to preserve probability).  In this case, we have\n",
    "\n",
    "$$\n",
    "S_{n+1} = M @ S_n \n",
    "$$\n",
    "\n",
    "Where the $@$ symbol is used to explicitly denote matrix multiplication.\n",
    "\n",
    "Since most clusters with transition to themselves, the diagonal values of the matrix can be quite high, and are typically removed.  Thus, for visualization, we remove the diagonal elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, index goes from 0 to 6 ... \n",
    "# can also have outlier groups in kmeans, need to check for this\n",
    "\n",
    "print(IDX.shape)\n",
    "print(np.min(IDX))\n",
    "print(np.max(IDX))\n",
    "\n",
    "IDX_max = np.max(IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mh.make_transition_matrix(IDX)\n",
    "print('\\n transition matrix:')\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder transition matrix, from most to least common cluster\n",
    "# diagonal elements monotonically decreasing\n",
    "\n",
    "IDX_ordered = mh.reorder_cluster(IDX, M)\n",
    "\n",
    "M = mh.make_transition_matrix(IDX_ordered)\n",
    "print('\\n transition matrix, ordered:')\n",
    "print(M)\n",
    "\n",
    "mh.image_M(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove diagonal, and make markov, for display\n",
    "\n",
    "print('transition matrix, diagonal elements removed, normalized (Markov)')\n",
    "\n",
    "np.fill_diagonal(M, 0)  # happens inplace\n",
    "M = mh.make_matrix_markov(M)\n",
    "\n",
    "print(M)\n",
    "mh.image_M(M, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
